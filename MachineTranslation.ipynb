{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pd.read_json(\"./data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.comments = [x[:40] for x in reddit.comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_col_ga = 'comments'\n",
    "\n",
    "df = reddit\n",
    "\n",
    "reddit = pd.DataFrame({\n",
    "     col:np.repeat(df[col].values, df[lst_col_ga].str.len())\n",
    "     for col in df.columns.drop(lst_col_ga)}\n",
    "   ).assign(**{lst_col_ga:np.concatenate(df[lst_col_ga].values)})[df.columns].dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit.comments = [x.replace(\"\\n\", '').replace(\"\\t\", '') for x in reddit.comments]\n",
    "reddit.title = [x.replace(\"\\n\", '').replace(\"\\t\", '') for x in reddit.title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit[['comments', 'title']].to_csv('reddit.csv', sep=\"\\t\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27430"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reddit.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_len = [len(x) for x in reddit.comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1423,   36,  263, ...,   24,  245,  190])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.clip(comm_len, a_min= 0, a_max=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.clip(comm_len, a_min= 0, a_max=2000), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n",
      "[i dont even know what to say rtoastme community you have all blown my mind and touched my heart when i posted this i was just sharing little bit of myself and didnt expect much response cause there are others who deserve love and are going through tough timesi absolutely did not expect to have people share so much love with me i didnt expect so many to tell me that a smile and a story was helping them with tough times i didnt expect to switch between smiling until my cheeks hurt and shedding loving tears because of the absolute love that was sent my way a beautiful soul gifted me with a gold award another gifted me with a platinum award i never thought anyone would ever think i deserved such things because they cost real money to gift i am astonished to everyone who has upvotes this commented such beautiful compliments and so many who shared their own stories and pains and achievements thank you all so freaking much i am humbled beyond words by you all i love each and every one of you and pray for everyone who is struggling in their lives it can get better it truly can and i hope everyone has their chances to enjoy life no matter whatyall have blown me away and i originally posted this picture and story as a toastme but i really want to toast all of you thank you for this it made my birthday better than it could have been it made it my best birthday ever] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday i love your smile] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday congratulations on making it to this huge milestone you must be a very strong determined person to have overcome this depression is so hard to deal with and anyone who comes out alive is a hero in my books im sure glad youre still around] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[it makes me happy to see you smile after everything you went throughwell done] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday im so glad that you are still here your smile looks like it could brighten a whole room] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday im so glad you could make it i hope you enjoy those little things that didnt make sense before people look at his face if he could make it you can this is important] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday and congratulations for living a year longer than you thought you set a new record for yourself and i hope you set many more your smile is infectious your eyes beautiful and your beard a healthy level of bushy which i appreciate] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[hooray and many happy returns on a milestone day you are an awesome bloke and have got through such adversity that we are incredibly lucky to have someone so vibrant as part of our communitystay being amazing] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[thats awesome to hear im glad youre still alive and kept fighting happy birthday i wish you the best] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday your smile is super contagious] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[youre such an awesome dude happy birthday man] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[hey hey hey buddy beautiful post and an even lovely attitude big hug on deciding to be in charge of your life and working every step to get where you are cheers on your bday and a toast to you from here in india rock on brother i hope you decide to have even bigger goals and smash em] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[wait wait hold up ref you actually found that strength to keep the fuck going hot damn son thats definite calls for celebration i love you i dont care how far away you are or how much i dont know you personally youre awesome because youve committed to a brilliant feat and im so fucking proud of you] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[this is amazing dude happy birthday im so glad youve discovered the light after such a dark time you are amazing and so so valuable im glad youre alive and sharing that smile with us today] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[hell yeah this gives me hope] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday dude you did it im proud of you] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday congratulations and keep on smiling] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[yesss i am so glad you are able to share your achievement with us happy birthday] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday hope you have a great day what an infectious smile] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[your smile is contagious happy birthday i hope everything works out as you wish it does] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[love that megawatt smile happy birthday mate keep smiling] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[looks great on you and i have no doubt that will look even better glad we get to celebrate with you] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[wish we could see more of that beard it looks glorious] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday toast stories like yours give me strength and hope for the future] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[glad youre here with us friend thanks for including all of us in your birthday celebration and dont forget about us for] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[i almost lost my best friend to suicide so its awesome to see that someone else won the battle there will always be people who deeply care about you and keep that genuine smile its gonna get you through the toughest of times] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[you are such a bright loving and happy soul im happy to see you thriving and doing so well it absolutely does get better and warriors like you are proof of it seeing your smile and hearing you share a piece of your story made my night may your life be filled with plenty of abundance and happiness] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[dude you look so genuinely happy and that shit is contagiouscongratulations on finding reasons to live and walking yourself into a better life no one else did that thats all you im in a similar place just recently hitting one year sober after i hit my bottom and planned some dark things i dont have the balls to make a public post like this at all be proud of yourself mate keep kicking butt edit how rude of me happy birthday] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[this is awesome im so happy for you happy birthday] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday thats amazing im so glad youre still around] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday i think the phrase life begins at has special meaning with you so i hope you enjoy every moment] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday and congratulations on your bravery and determination so happy for you] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[look at yourself what a strong person you are youre great and powerful and you should celebrate the biggest party ever its so lovely how happy you look be proud of yourself you won the fight and the whole community is celebrating you] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[honestly good on you good on you to the moon and back you deserve as much happiness as you want and nobody should ever stop you goodluck man] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[im a happy go lucky kind of person ive been through shit and always came out clean on the other side i have no idea what depression feels like but i know from family what it can do seeing the pure happiness in your eyes is such a heart warming sight] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[legend] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday and i really love your smile] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[you are a survivor my man you decided to see whats hiding in your future and trust me since you have the opportunity to live to tell and remember knowing stuff is power the rest is extraim proud of you even if i will never will have the honor to shake your hand and hug you] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[you are amazing all the best] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[happy birthday its so reassuring to know that one can get out of that dreaded mental pit even after so longmay your story provide strength to lots of people who find themselves trapped in depression] => [i wanted to share a big milestone for me when i was deep in my depression for over a decade i had planned to disappear and quietly commit suicide at age i battled depression and found reasons to live today i turned and can now assure my past self that it does get better toast if you want]\n",
      "[dude i like what i see on my screen right now you look like a really nice and gentle guy with really kind eyes here is a list of things i really like about you know how to prepare a mean charcuterie board you seem to be a very thoughtful and intelligent guy you seem to have a great sense of humour you absolutely rock that beard you love pirates you love found out about your existence minutes ago and i already found so many things to like about you and im damn sure that i would fine more reasons to like you if i knew you a bit better stay strong my dude and never forget that youre awesome and important] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[takes courage to post stuff on reddit your courage is giving me courage so thank you sending positive vibes your wayand ill quote one of my favorite stoic quotes i judge you unfortunate because you have never lived through misfortune you have passed through life without an opponentno one can ever know what you are capable of not even you seneca] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you have very kind eyes use that kindness but do your best to save some of it for yourself] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you look like you could make some awesome pancakes my dude] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you are worthy of love happiness and peace even if you cant yet see it sending you positive vibes and reminding you that you will get through this day its tough out here try and be kind to yourself as you truly deserve it] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[not gonna lie this is probably my favorite pic on this sub people like this guy make me want to suggest we have like a massive hang out like at a beach or something to chill in a massive tiki hut with like hawaiian bbq or something oh man oh man like an oktoberfest in helens ga but at the beachwe could do like daytona or something boardwalk and ocean by day night time is dance tiki torches and massive hammocks where we talk about stuff that bothers us and stuff that helps were reddit crowd so you know people are going to have quick quips quick quips and pineapples by the bonfire oh dang grilled pineappleyo mods we need to do thishow big do black light bulbs get would we be able to do a couple beach volleyball courts with neon and black lighting and pineappleby the fire] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you look like a cool dude to hang out with the kinda dude that has people always chuckling] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[bro what in the flip is there to not like in that mirror is it a big blur did you just get out of the shower or somethingyour hair lines are gahtdam perfect scalp and that beard line man thats some perfect beard line] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[without wanting to sound weird you have stunning eyes i could look into for hours theyre just perfectthe rest of your face is damn good looking too but those eyestomorrows another day it doesnt have to be perfect but it is a whole new start be kind to yourself] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[did a little digging and i know for a fact you are strong you have been through so much already and can pull through again and again no doubt those kind eyes have been subject to many trials but when push comes to shove you have such great compassion know that you are worth the happiness that you grant others chin up friend youve got this] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[bad mental health days dont last think of it as a storm passing entrance it and know it will be over you rock a bad ass beard my dude looking good] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[keep your chin up babe take it day by day make sure to do something you enjoy todayall the best x] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you have a kind handsome face mental health really can suck the life out of us i know that feeling but dont let the voice inside your head win you are worth more than that] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[i really like your face there is a gentleness and soulfulness about you that makes me feel calm inside it takes a lot of courage on days like today to be a good friend and a source of refuge to yourself luckily you have all the courage and love necessary to not only be kind to yourself but also to shower others with your openhearted kindness as well i know that its not easy to take what people say about you seriously on the internet but i promise with all the sincerity i can muster that i believe in you man my name is justin and youll always have a friend in me hug] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[i dont know you but your face has a certain warmth to it you just seem like a trustworthy actually decent dude and reading the comments everyone seems to agree plus you rock a beard not a feat everyone can claim] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[oh my gosh i can literally feel your tender spirit and empathy through the screen it would be an understatement to say my dad is not physically well and i have been on the verge of crying for weeks your eyes give me such a sense on peace and understanding thank you for just being you and remember this too shall pass you are worth it and make a difference in the lives of those around you] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you look so easy to approach you seem really sweet] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you look like you would give killer hugs love your eyes too i get the feeling i could listen to you for a long time i bet you have some stories to tell] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[your beautiful and your eyes are breath taking i swear i just felt my heart skip a beat] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[well shoot i think youre pretty handsome and kind looking may sound silly but try making a little lists of your good qualities and other little things u appreciate rn no matter how small its okay to not feel okay u are loved and supported] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you have great eyes my immediate impression was seems like a super kind chilled dude who i would love to hang out with and that charcuterie board watching the hobbit yeah thats my kind of night in too man hope you have find a so who enjoys it all the same you most definitely deserve to be happy] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you have really nice eyes and a kind face but not the annoying kind face rather the i want you ob my team kind face] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you are undefeated king] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you look like you give excellent hugs] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[all we can do is our best giving you a virtual hug hang in there my friend] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[hello im really sorry to hear youre having a bad mental health day but for what its worth this internet stranger thinks you have very kind eyes and she hopes your day picks up for you] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you are a good looking guy with a lot of potential in that beard so ill guess this isnt physically motivated maybe you just need someone to talk to either way it gets better and you look like a survivor id love to have a friend like you so keep your head up] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you might not like what you see in the mirror but i definitely like what i see your skin looks so good i hope it gets better for you so you can smile again] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[friend your eyes arent telling you the truth right now i see a kind faced gentleman with deep knowing eyes a very good friend of mine reminds me when im struggling depression lies itd really damned hard to navigate life when you cant trust what your own mind is telling you good on you for seeking some perspective] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you have these nice puppydoglike eyes you have an amazing afroyou look like a genuinely nice person who would probably donate to charities] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[i like your shirt] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you look like the type that i could have infinite amounts of conversations on our thoughts and beliefs while maintaining mutual respect for one another difference in opinions something that i wish i could see in more people these days keep your head up brother many of us go through the same types of days we just have to lift each other up take it easy man] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[i would try to say something funny and that says a lot because my english is not even all that thats how down to earth you seem to me] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[dude you are rocking that beard for a start props to a fellow beardy broand hey look at you being open enough to admit youre not ok and asking for a little help i know how hard that is myself which puts you a fair way ahead of quite a few guys] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[youre here and thats a start glad you reached out that says a lot about you proud of you for starting there also my phone wanted me to post the avocado emoji twice for no reason so here it is] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you have the nicest eyes its easy to see youre a gentle soul im in to that orange shirt its my favorite color and it looks good on you your beard and hair rock i like what i see hugs to you and i hope things get better for you] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[i would love to hang out with you you have the nicest eyes and youre very handsome rocking that beard take care pal stay cool] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[i like your handwriting and everything everyones has said i agreed with before i read them] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[truth be told none of us really like what we see in the mirror that includes that girl or guy you think is absolutely perfect in every single way you only see them as perfect because you dont see their flaws just like how not everyone may see the flaws that you see in yourself assuming the flaws you see are flaws at all remember comparison is the thief of joy] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[you might need a new mirror because what im seeing looks like a good dude someone id like to drink a beer or play xbox with or both] => [having a bad mental health day not liking what i see in the mirror really need some positive]\n",
      "[youre beautiful really very beautiful] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[you remind me of annie lennox she is badass and confident and that to me is hot i see the same in you go forward and rock the day] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[in a black dress youd look like a neoaudrey hepburn about as feminine as they come] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[im an oncology rn i am thankful you are here may you find hope amidst the grief you are strong and you are a gift] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[you are absolutely unequivocally stunning your bone structure is enviable theres no question that youre feminine and youre strong bc youre still here and still keeping on keeping on i think youre amazing] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[just in case you ever feel like this again remember the sun will never see how beautiful it is to us you are amazing and dont let anyone tell you otherwise] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[you are pretty af tbh] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[your hair looks damn good short i like the septum ring a lot too good combo you look very feminine to me] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[your femininity can never be taken away from you not by a disease a person not anything you are you a beautiful young lady with so many years on this planet to shine as bright as you possibly can keep at it youre doing a great job] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[you have a wonderful face structure and mad makeup skills id be straight for u] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[you have the most beautiful eyes ive ever seen you look intelligent and enchanting high cheekbones makeup that enhances the gorgeous features already there your choice in clothing is feminine and sophisticated anyone would look at you and see a beautiful girl] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[honestly not usually a fan of short hair on most girls but you really pull it off shows how gorgeous your eyes are also your brows are amazing keep on fighting beautiful youre so much stronger than this] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[some people might not think rosie the riveter seems feminine but to a lot of people shes feminine and a toughass chickwhat youve been throughyou sound like a toughass chick too congratulations on being stronger than everyone i know except my mother who also lost her hair and breasts to cancershe can probably beat up you and me and everyone else here with one arm tied behind her back as the others have pointed out though youre prettier than my mom sorry mom] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[although appearance isnt everything i too feel the same way very frequently i think you are attractive in a strong lasting sort of way and you have a talent with makeup you wont need a lot but you use it well] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[first look youre all woman read title okaysecond look nope was right the first timewell done and keep going] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[to others losing those physical features wont mean youve lost your femininity but im sorry youre struggling with this youre beautiful im a womanlovingwoman and i would never feel my partner was less feminine because her breasts were removed or she otherwise didnt have any femininity is what we make of it how we dress how we carry ourselves you definitely have the feminine vibe in this photograph] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[you are stunning] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[heres some light toast for you stay strong sending a virtual fist bump bump] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[youre the snack that fought back i have a few friends that have short hair like yours more a fashion choice than anything and theyre having some of the same feelings the people who say short hair isnt feminine obviously havent seen you youre feminimity isnt tied to your hair or your body its in how you express yourself and from everything i see your a strong beautiful woman who dont take no shit keep on doing you girl cuz the world needs a whole lot more of it] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n",
      "[such stunning eyes youre a beautiful strong woman you are feminine even if you cant see it yet you are your body sometimes has to change in order to win the fight you are still so damn radiant and breathtakingi hope you have a great day] => [lost my hair and my breasts to cancer last year have a hard time feeling feminine now this week has been particularly hard and i feel really alone would appreciate a light toasting]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\tpairs = [line.split('\\t') for line in  lines]\n",
    "\treturn pairs\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor pair in lines:\n",
    "\t\tclean_pair = list()\n",
    "\t\tfor line in pair:\n",
    "\t\t\t# normalize unicode characters\n",
    "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\t\tline = line.decode('UTF-8')\n",
    "\t\t\t# tokenize on white space\n",
    "\t\t\tline = line.split()\n",
    "\t\t\t# convert to lowercase\n",
    "\t\t\tline = [word.lower() for word in line]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tline = [word.translate(table) for word in line]\n",
    "\t\t\t# remove non-printable chars form each token\n",
    "\t\t\tline = [re_print.sub('', w) for w in line]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\tline = [word for word in line if word.isalpha()]\n",
    "# \t\t\tstore as string\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\t\tif len(clean_pair) == 2:\n",
    "\t\t\t cleaned.append(clean_pair)\n",
    "\treturn array(cleaned)\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "filename = './reddit.csv'\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'english-german.pkl')\n",
    "# spot check\n",
    "for i in range(100):\n",
    "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['i actually really like your smile it makes me happy',\n",
       "       'quite new to reddit thought this might be a good place to start been battlingcoming to terms with dysmorphiacould use a pickmeup'],\n",
       "      dtype='<U7837')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pairs[21404]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([len(x) for x in clean_pairs if len(x) == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Reduce, and Split Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# # save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset =  clean_pairs   #raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:20000], dataset[20000:]\n",
    "# save\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '../../../ExamStudy/data/glove.6B/'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100  # We will cut reviews after 100 words\n",
    "training_samples = 20000  # We will be training on 200 samples\n",
    "validation_samples = 5000  # We will be validating on 10000 samples\n",
    "max_words = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(dataset[:, 0:2].ravel())\n",
    "sequences = tokenizer.texts_to_sequences(dataset[:, 0:2].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24568 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 24569\n",
      "English Max Length: 1000\n",
      "German Vocabulary Size: 24569\n",
      "German Max Length: 1000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y\n",
    "\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    \n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True, \n",
    "                        weights = [embedding_matrix], trainable = False))\n",
    "#     model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "# eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(tokenizer.word_index) + 1\n",
    "eng_length = 1000#max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "# ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(tokenizer.word_index) + 1\n",
    "ger_length = 1000#max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 128)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_model(model, to_file='model.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=1, batch_size=64, validation_data=(testX, testY), \n",
    "          callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[listen man i have no reason to lie it doesnt benefit me in any way you are attractive you are and i have no doubt that someone else will find you or vice versa keep trying youll get there], target=[i was beginning to conquer my depression and i had the biggest support from my boyfriend but we broke up and ive been spiralling since anxiety and insecurities kick me down and tell me i wont find love and wont get better some toasting would be much appreciated], predicted=[]\n",
      "src=[breakups are hard but you need to accept that people come and go whatever happens its for the good so just stay tuned and keep going you look so strong i know you can get over this], target=[feeling pathetic even as i write this going thru an awful break up and the anxiety is overwhelming just want to feel that im worth something just want a distraction to get me thru today could you some toasting thank you in advance], predicted=[]\n",
      "src=[you look lovely can i ask what people are saying thats upsetting you], target=[someone on reddit is really upsetting and triggering me right now and i just need some reassurance that everything is okay old picture sorry], predicted=[]\n",
      "src=[youre a good looking dude nice haircut], target=[uneventful birthday so far though id kill two birds with one stone], predicted=[]\n",
      "src=[ohmyfuckinggoodness i want your hair], target=[struggling a bit could use some positivity], predicted=[]\n",
      "src=[if we are talking high school it sucks and is almost over you can have whatever year you want and start your life fresh again in months the best part of life is ahead of you every day you will get to do more and more of what you love and less and less of what bothers you enjoy], target=[senior year is starting and im both exhausted and extremely scared so could really use a self esteem boost to start the day], predicted=[]\n",
      "src=[cute glasses], target=[french medstudent stressed out tired and anxious about the future needs a little toast to help brighten her day], predicted=[]\n",
      "src=[dude i feel this on a spiritual level i havent had a partner in years and i dont have any friends really but i think the best thing you can do for yourself is to find a sort of peace being on your own if that makes sense people come and go and i think the ones that are truly meant to stay in your life will it might take five days or five years but youll find those people for now just do you and enjoy your own company learn to love yourself honestly though youre really cute and you look like sodone id happily chill with on a friday night for late night drinks and chatter and stuff chin up mdude], target=[havent had a girlfriend in years have like friends if that im and still havent figured out why people dont like me i would appreciate anything to help pick me up], predicted=[]\n",
      "src=[love love love the color of your hair], target=[ive had a very rough past months going through a process of rebuilding myself emotionally i would love some toasting thanks so much], predicted=[]\n",
      "src=[you will heal my dude it will take time but you will heal from this stay steady], target=[a week since i made a post having a hard time believing she wants a divorceletting go cant focus on anything else just trying to get thru the day until my therapy session tomorrow can use any kind words], predicted=[]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6678a683eec0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# test on some training sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;31m# test on some test sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-6678a683eec0>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, tokenizer, sources, raw_dataset)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# translate encoded source text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meng_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0mraw_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-6678a683eec0>\u001b[0m in \u001b[0;36mpredict_sequence\u001b[0;34m(model, tokenizer, source)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mintegers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintegers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-6678a683eec0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mintegers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintegers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/menv/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \"\"\"\n\u001b[0;32m-> 1037\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/menv/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "\tactual, predicted = list(), list()\n",
    "\tfor i, source in enumerate(sources):\n",
    "\t\t# translate encoded source text\n",
    "\t\tsource = source.reshape((1, source.shape[0]))\n",
    "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
    "\t\traw_target, raw_src = raw_dataset[i]\n",
    "\t\tif i < 10:\n",
    "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "\t\tactual.append([raw_target.split()])\n",
    "\t\tpredicted.append(translation.split())\n",
    "\t# calculate BLEU score\n",
    "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "\n",
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tprint(prediction)\n",
    "\tintegers = [np.argmax(vector) for vector in prediction]\n",
    "\tprint(integers)\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_txt = [\"hello i am sad today need some kind words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_txt = encode_sequences(eng_tokenizer, 1531, sample_pred_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.67725769e-02 2.28690691e-02 1.57457013e-02 ... 2.43741597e-05\n",
      "  3.27524285e-05 2.78582065e-05]\n",
      " [1.32933974e-01 3.84270027e-02 2.82789599e-02 ... 7.85659449e-06\n",
      "  1.06466805e-05 9.53474319e-06]\n",
      " [1.72415331e-01 3.75424102e-02 3.06620076e-02 ... 6.18893455e-06\n",
      "  8.05057425e-06 7.75016997e-06]\n",
      " ...\n",
      " [6.26682818e-01 1.45009588e-02 1.07178641e-02 ... 3.06589095e-06\n",
      "  2.81753341e-06 3.56833925e-06]\n",
      " [6.26792729e-01 1.44970333e-02 1.07142255e-02 ... 3.06507218e-06\n",
      "  2.81687471e-06 3.56710007e-06]\n",
      " [6.26892865e-01 1.44934766e-02 1.07109342e-02 ... 3.06433412e-06\n",
      "  2.81627968e-06 3.56597616e-06]]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sequence(model, eng_tokenizer, sample_pred_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 190ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[4.67725769e-02, 2.28690691e-02, 1.57457013e-02, ...,\n",
       "         2.43741597e-05, 3.27524285e-05, 2.78582065e-05],\n",
       "        [1.32933974e-01, 3.84270027e-02, 2.82789599e-02, ...,\n",
       "         7.85659449e-06, 1.06466805e-05, 9.53474319e-06],\n",
       "        [1.72415331e-01, 3.75424102e-02, 3.06620076e-02, ...,\n",
       "         6.18893455e-06, 8.05057425e-06, 7.75016997e-06],\n",
       "        ...,\n",
       "        [6.26682818e-01, 1.45009588e-02, 1.07178641e-02, ...,\n",
       "         3.06589095e-06, 2.81753341e-06, 3.56833925e-06],\n",
       "        [6.26792729e-01, 1.44970333e-02, 1.07142255e-02, ...,\n",
       "         3.06507218e-06, 2.81687471e-06, 3.56710007e-06],\n",
       "        [6.26892865e-01, 1.44934766e-02, 1.07109342e-02, ...,\n",
       "         3.06433412e-06, 2.81627968e-06, 3.56597616e-06]]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sample_pred_txt, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much effort is put into data preparation vs. actually training and evaluating the model?\n",
    "In the the tutorial that we followed, not too much effort was put into data preparation. Most of the effort was simply lowercasing, removing punctuation, removing non-printable characters, and removing numbers. We could do a lot more of data preparation by trying to lemmatize/stem, finding POS tags, getting the dependency parsing, and more. Training and evaluating the model takes the bulk of the effort in this tutorial, as we need to worry about how to create the model and using the BLEU score to evaluate it. \n",
    "\n",
    "### What constitutes the input features? What constitutes the top layer?\n",
    "The input features are made up of tokenized German words. Each word in an example is converted into an arbitrary list of numbers by the Keras Tokenizer. The output is also a list of tokenized words, but these words are in English and then they are also one-hot encoded so they work with the softmax in the model. \n",
    "\n",
    "### What does it mean that this is a \"generative\" classifier? How is that different from a discriminative classifier?\n",
    "A generative classifier creates output based on both the input data and the previous outputs of the model. This is necessary for our LSTM because it has to generate word w based on word w-1, which it also predicted. A discriminative classifer generates output only based on the input data, and \n",
    "\n",
    "### How do you know your model works? What does a BLEU metric tell you? Can you think of a better way to evaluate MT output?\n",
    "I know that my model works based on the BLEU metrics. This metric tells me how closely the predicted translation matches the target translation. This is not an exact match score, but it gives credit for a translation that is mostly the same. The higher the BLEU score, the better the predicted translation matches the target translation. There are different BLEU metrics for the different n-grams (BLEU-1, BLEU-2, etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "Assignment: A8 Machine Translation with Keras\n",
      "OK, version v1.13.11\n",
      "=====================================================================\n",
      "\n",
      "Successfully logged in as DanieleMoro@u.boisestate.edu\n"
     ]
    }
   ],
   "source": [
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('a8.ok')\n",
    "ok.auth(inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_notebook();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving notebook... Saved 'A8-machine-translation-keras.ipynb'.\n",
      "Submit... 0.0% complete\n",
      "Could not submit: Assignment does not exist\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:menv]",
   "language": "python",
   "name": "conda-env-menv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
